{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc4c61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 19:50:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/03 19:51:37 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "'''\n",
    "数据分析\n",
    "'''\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#分布式读取训练数据。\n",
    "train_images_spark = spark.read.format(\"image\").load(\"../Datasets/cifar_100/imgs/train/\")\n",
    "train_images = train_images_spark.select('image.origin', 'image.data').toPandas()\n",
    "train_images['Image'] = train_images['origin'].map(lambda x: x[73:])\n",
    "train_images = train_images.set_index('Image')\n",
    "\n",
    "train_labels = pd.read_csv('../Datasets/cifar_100/cifar100_train.csv', index_col='Image')\n",
    "train_data = train_images.join(train_labels)[['data', 'Label']]\n",
    "\n",
    "#分布式读取测试数据。\n",
    "test_images_spark = spark.read.format('image').load('../Datasets/cifar_100/imgs/test/')\n",
    "test_images = test_images_spark.select('image.origin', 'image.data').toPandas()\n",
    "test_images['Image'] = test_images['origin'].map(lambda x: x[73:])\n",
    "test_images = test_images.set_index('Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47b9455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 50000 entries, imgs/train/computer_keyboard_s_000712.png to imgs/train/adriatic_s_001723.png\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   data    50000 non-null  object\n",
      " 1   Label   50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8f046a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 10000 entries, imgs/test/computer_keyboard_s_002225.png to imgs/test/beer_bottle_s_000236.png\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   origin  10000 non-null  object\n",
      " 1   data    10000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 234.4+ KB\n"
     ]
    }
   ],
   "source": [
    "test_images.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e0e0db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "数据预处理\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#抽取训练和测试集中的图像特征。\n",
    "X_train = np.array(np.vstack([np.array(feature).astype('float32') for feature in train_data['data'].values]))\n",
    "X_test = np.array(np.vstack([np.array(feature).astype('float32') for feature in test_images['data'].values]))\n",
    "\n",
    "#将图像的类别标签进行编码。\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train_data['Label'])\n",
    "\n",
    "#将训练和测试集中的图像特征进行标准化处理。\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "\n",
    "#将训练集拆分为训练和验证集。\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9338c628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sysctl: unknown oid 'machdep.cpu.leaf7_features'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "采用卷积神经网络，并且在验证集上进行模型寻优。\n",
    "'''\n",
    "import paddle\n",
    "from paddle import nn, optimizer, metric\n",
    "\n",
    "\n",
    "#设定超参数。\n",
    "NUM_CLASSES = 100\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# 搭建卷积神经网络。\n",
    "paddle_model = nn.Sequential(\n",
    "    nn.Conv2D(in_channels=3, out_channels=32, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2D(kernel_size=2, stride=2),\n",
    "    \n",
    "    nn.Conv2D(in_channels=32, out_channels=64, kernel_size=3),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2D(kernel_size=2, stride=2),\n",
    "    \n",
    "    nn.Conv2D(in_channels=64, out_channels=64, kernel_size=3),\n",
    "    \n",
    "    nn.Flatten(),\n",
    "    \n",
    "    nn.Dropout(DROPOUT_RATE),\n",
    "    \n",
    "    nn.Linear(in_features=1024, out_features=NUM_CLASSES),\n",
    ")\n",
    "\n",
    "#初始化卷积神经网络模型。\n",
    "model = paddle.Model(paddle_model)\n",
    "\n",
    "# 为模型训练做准备，设置优化器，损失函数和评估指标。\n",
    "model.prepare(optimizer=optimizer.Adam(learning_rate=LEARNING_RATE, parameters=model.parameters()),\n",
    "              loss=nn.CrossEntropyLoss(),\n",
    "              metrics=metric.Accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6b36152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python_paddle/lib/python3.8/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 625/625 [==============================] - loss: 3.8982 - acc: 0.0793 - 88ms/step          \n",
      "save checkpoint at /Users/michael_fan/ML-Kaggle-2nd-Edition-Gitee/Checkpoints/cifar_100/0\n",
      "Eval begin...\n",
      "step 157/157 [==============================] - loss: 4.0221 - acc: 0.1086 - 28ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 2/5\n",
      "step 625/625 [==============================] - loss: 3.5791 - acc: 0.1594 - 89ms/step          \n",
      "save checkpoint at /Users/michael_fan/ML-Kaggle-2nd-Edition-Gitee/Checkpoints/cifar_100/1\n",
      "Eval begin...\n",
      "step 157/157 [==============================] - loss: 3.6682 - acc: 0.1540 - 28ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 3/5\n",
      "step 625/625 [==============================] - loss: 3.3321 - acc: 0.1978 - 89ms/step          \n",
      "save checkpoint at /Users/michael_fan/ML-Kaggle-2nd-Edition-Gitee/Checkpoints/cifar_100/2\n",
      "Eval begin...\n",
      "step 157/157 [==============================] - loss: 3.4163 - acc: 0.1734 - 28ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 4/5\n",
      "step 625/625 [==============================] - loss: 3.4375 - acc: 0.2296 - 89ms/step          \n",
      "save checkpoint at /Users/michael_fan/ML-Kaggle-2nd-Edition-Gitee/Checkpoints/cifar_100/3\n",
      "Eval begin...\n",
      "step 157/157 [==============================] - loss: 3.3282 - acc: 0.1864 - 29ms/step          \n",
      "Eval samples: 10000\n",
      "Epoch 5/5\n",
      "step 625/625 [==============================] - loss: 3.0715 - acc: 0.2477 - 90ms/step          \n",
      "save checkpoint at /Users/michael_fan/ML-Kaggle-2nd-Edition-Gitee/Checkpoints/cifar_100/4\n",
      "Eval begin...\n",
      "step 157/157 [==============================] - loss: 3.3313 - acc: 0.1896 - 29ms/step          \n",
      "Eval samples: 10000\n",
      "save checkpoint at /Users/michael_fan/ML-Kaggle-2nd-Edition-Gitee/Checkpoints/cifar_100/final\n"
     ]
    }
   ],
   "source": [
    "from paddle.io import TensorDataset\n",
    "\n",
    "\n",
    "X_train = X_train.reshape([-1, 3, 32, 32])\n",
    "X_train = paddle.to_tensor(X_train)\n",
    "train_dataset = TensorDataset([X_train, y_train])\n",
    "\n",
    "X_val = X_val.reshape([-1, 3, 32, 32])\n",
    "X_val = paddle.to_tensor(X_val)\n",
    "val_dataset = TensorDataset([X_val, y_val])\n",
    "\n",
    "#模型训练与寻优。\n",
    "model.fit(train_dataset, val_dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, save_dir='../Checkpoints/cifar_100', verbose=1)\n",
    "\n",
    "#保存在验证集上表现最优的模型。\n",
    "model.save('../Checkpoints/cifar_100/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09d9d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict begin...\n",
      "step 10000/10000 [==============================] - 2ms/step          \n",
      "Predict samples: 10000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "使用最优的模型，依据测试数据的特征进行类别预测。\n",
    "'''\n",
    "X_test = X_test.reshape([-1, 3, 32, 32])\n",
    "X_test = paddle.to_tensor(X_test)\n",
    "test_dataset = TensorDataset([X_test])\n",
    "\n",
    "model.load('../Checkpoints/cifar_100/test')\n",
    "results = model.predict(test_dataset)\n",
    "\n",
    "predictions = le.inverse_transform([np.argmax(item[0]) for item in results[0]])\n",
    "test_images['Prediction'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b94f633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = pd.read_csv('../Datasets/cifar_100/cifar100_test.csv', index_col='Image')\n",
    "\n",
    "submission_df = test_images.join(test_ids)\n",
    "\n",
    "submission_df['Label'] = submission_df['Prediction']\n",
    "\n",
    "submission_df['Label'].to_csv('../Kaggle_submissions/cifar100_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6346a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
